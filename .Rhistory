this.tokens.in <- tokens(int.corpus,
remove_numbers = TRUE,
remove_separators = TRUE,
remove_punct = TRUE)
this.tokens.in<- tokens_select(this.tokens.in, stopwords('english'), selection='remove')
this.tok.in.dfm <- dfm(this.tokens.in)
library("stm")
topic.count <- 10
tok.dfm.trim <- dfm_trim(this.tok.in.dfm, min_docfreq = 0.075, max_docfreq = 0.90, docfreq_type = "prop") # min 7.5% / max 95%
dfm2stm <- convert(dfm.trim, to = "stm")
model.stm <- stm(dfm2stm$documents, dfm2stm$vocab, K = topic.count, data = dfm2stm$meta, init.type = "Spectral")
data.frame(t(labelTopics(model.stm, n = 10)$prob))
plot(model.stm, type = "summary", text.cex = 0.5)
library("stm")
topic.count <- 15
tok.dfm.trim <- dfm_trim(this.tok.in.dfm, min_docfreq = 0.075, max_docfreq = 0.90, docfreq_type = "prop") # min 7.5% / max 95%
dfm2stm <- convert(dfm.trim, to = "stm")
model.stm <- stm(dfm2stm$documents, dfm2stm$vocab, K = topic.count, data = dfm2stm$meta, init.type = "Spectral")
data.frame(t(labelTopics(model.stm, n = 10)$prob))
this.tokens.in <- tokens(int.corpus,
remove = c('okay'),
remove_separators = TRUE,
remove_punct = TRUE)
this.tokens.in<- tokens_select(this.tokens.in, 'okay', selection='remove')
this.tok.in.dfm <- dfm(this.tokens.in)
this.tok.in.dfm <- dfm(this.tokens.in)
this.tokens.in <- tokens(int.corpus,
remove_numbers = TRUE,
remove_separators = TRUE,
remove_punct = TRUE)
this.tokens.in<- tokens_select(this.tokens.in, stopwords('english'), selection='remove')
this.tokens.in<- tokens_select(this.tokens.in, 'okay', selection='remove')
this.tok.in.dfm <- dfm(this.tokens.in)
library("stm")
topic.count <- 15
tok.dfm.trim <- dfm_trim(this.tok.in.dfm, min_docfreq = 0.075, max_docfreq = 0.90, docfreq_type = "prop") # min 7.5% / max 95%
dfm2stm <- convert(dfm.trim, to = "stm")
model.stm <- stm(dfm2stm$documents, dfm2stm$vocab, K = topic.count, data = dfm2stm$meta, init.type = "Spectral")
data.frame(t(labelTopics(model.stm, n = 10)$prob))
this.tokens.in <- tokens(int.corpus,
remove_numbers = TRUE,
remove_separators = TRUE,
remove_punct = TRUE)
this.tokens.in<- tokens_select(this.tokens.in, stopwords('english'), selection='remove')
this.tok.in.dfm <- this.tokekns.in %>% dfm(remove = 'okay')
this.tokens.in <- tokens(int.corpus,
remove_numbers = TRUE,
remove_separators = TRUE,
remove_punct = TRUE)
this.tokens.in<- tokens_select(this.tokens.in, stopwords('english'), selection='remove')
this.tok.in.dfm <- this.tokens.in %>% dfm(remove = 'okay')
library("stm")
topic.count <- 15
tok.dfm.trim <- dfm_trim(this.tok.in.dfm, min_docfreq = 0.075, max_docfreq = 0.90, docfreq_type = "prop") # min 7.5% / max 95%
dfm2stm <- convert(dfm.trim, to = "stm")
model.stm <- stm(dfm2stm$documents, dfm2stm$vocab, K = topic.count, data = dfm2stm$meta, init.type = "Spectral")
data.frame(t(labelTopics(model.stm, n = 10)$prob))
plot(model.stm, type = "summary", text.cex = 0.5)
plot(model.stm, type = "perspectives", topics = c(3,1))
library("stm")
topic.count <- 15
tok.dfm.trim <- dfm_trim(this.tok.in.dfm, min_docfreq = 0.075, max_docfreq = 0.90, docfreq_type = "prop") # min 7.5% / max 95%
dfm2stm <- convert(tok.dfm.trim, to = "stm")
model.stm <- stm(dfm2stm$documents, dfm2stm$vocab, K = topic.count, data = dfm2stm$meta, init.type = "Spectral")
data.frame(t(labelTopics(model.stm, n = 10)$prob))
library("stm")
topic.count <- 15
tok.dfm.trim <- dfm_trim(this.tok.in.dfm, min_docfreq = 0.075, max_docfreq = 0.90, docfreq_type = "prop") # min 7.5% / max 95%
dfm2stm <- convert(tok.dfm.trim, to = "stm")
model.stm <- stm(dfm2stm$documents, dfm2stm$vocab, K = topic.count, data = dfm2stm$meta, init.type = "Spectral")
data.frame(t(labelTopics(model.stm, n = 1)$prob))
library("stm")
topic.count <- 5
tok.dfm.trim <- dfm_trim(this.tok.in.dfm, min_docfreq = 0.075, max_docfreq = 0.90, docfreq_type = "prop") # min 7.5% / max 95%
dfm2stm <- convert(tok.dfm.trim, to = "stm")
model.stm <- stm(dfm2stm$documents, dfm2stm$vocab, K = topic.count, data = dfm2stm$meta, init.type = "Spectral")
data.frame(t(labelTopics(model.stm, n = 1)$prob))
plot(model.stm, type = "summary", text.cex = 0.5)
library("stm")
topic.count <- 8
tok.dfm.trim <- dfm_trim(this.tok.in.dfm, min_docfreq = 0.075, max_docfreq = 0.90, docfreq_type = "prop") # min 7.5% / max 95%
dfm2stm <- convert(tok.dfm.trim, to = "stm")
model.stm <- stm(dfm2stm$documents, dfm2stm$vocab, K = topic.count, data = dfm2stm$meta, init.type = "Spectral")
data.frame(t(labelTopics(model.stm, n = 1)$prob))
library("stm")
topic.count <- 8
tok.dfm.trim <- dfm_trim(this.tok.in.dfm, min_docfreq = 0.075, max_docfreq = 0.90, docfreq_type = "prop") # min 7.5% / max 95%
dfm2stm <- convert(tok.dfm.trim, to = "stm")
model.stm <- stm(dfm2stm$documents, dfm2stm$vocab, K = topic.count, data = dfm2stm$meta, init.type = "Spectral")
data.frame(t(labelTopics(model.stm, n = 1)$prob))
plot(model.stm, type = "summary", text.cex = 0.5)
plot(model.stm, type = "perspectives", topics = c(1,5))
library("stm")
dfm2stm
str(dfm2stm)
str(dfm2stm$vocab)
glimpse(dfm2stm$vocab)
glimpse(dfm2stm$documents)
glimpse(dfm2stm$meta)
library("stm")
topic.count <- 8
tok.dfm.trim <- dfm_trim(this.tok.in.dfm, min_docfreq = 0.075, max_docfreq = 0.90, docfreq_type = "prop") # min 7.5% / max 95%
dfm2stm <- convert(tok.dfm.trim, to = "stm")
model.stm <- stm(dfm2stm$documents, dfm2stm$vocab, K = topic.count, data = dfm2stm$meta, init.type = "Spectral")
library("stm")
topic.count <- 6
tok.dfm.trim <- dfm_trim(this.tok.in.dfm, min_docfreq = 0.075, max_docfreq = 0.90, docfreq_type = "prop") # min 7.5% / max 95%
dfm2stm <- convert(tok.dfm.trim, to = "stm")
model.stm <- stm(dfm2stm$documents, dfm2stm$vocab, K = topic.count, data = dfm2stm$meta, init.type = "Spectral")
plot(model.stm, type = "perspectives", topics = c(1,5))
plot(model.stm, n = 6, type = "perspectives", topics = c(1,5))
plot(model.stm, n = 10, type = "perspectives", topics = c(1,5))
library( pagedown )
install.packages("pagedown")
install.packages("pagedown")
library( pagedown )
library( glue )
library( tidyverse )
library( pander )
URL <- "https://raw.githubusercontent.com/DS4PS/cv/master/positions.csv"
position_data <- read_csv( URL )
head( position_data ) %>% pander()
table( position_data$section ) %>% knitr::kable( align="l" )
source( "https://raw.githubusercontent.com/DS4PS/cv/master/parsing_functions.R" )
ls()
source( "https://raw.githubusercontent.com/DS4PS/cv/master/parsing_functions.R" )
ls()
ls()
library( pagedown )
library( glue )
library( tidyverse )
library( pander )
URL <- "https://raw.githubusercontent.com/DS4PS/cv/master/positions.csv"
position_data <- read_csv( URL )
head( position_data ) %>% pander()
table( position_data$section ) %>% knitr::kable( align="l" )
source( "https://raw.githubusercontent.com/DS4PS/cv/master/parsing_functions.R" )
ls()
print_section <- function( position_data, section_id ){
position_data %>%
filter(section == section_id) %>%
arrange(desc(end)) %>%
mutate(id = 1:n()) %>%
pivot_longer(
starts_with('description'),
names_to = 'description_num',
values_to = 'description'
) %>%
filter(!is.na(description) | description_num == 'description_1') %>%
group_by(id) %>%
mutate(
descriptions = list(description),
no_descriptions = is.na(first(description))
) %>%
ungroup() %>%
filter(description_num == 'description_1') %>%
mutate(
timeline = ifelse(
is.na(start) | start == end,
end,
glue('{end} - {start}')
),
description_bullets = ifelse(
no_descriptions,
' ',
map_chr(descriptions, ~paste('-', ., collapse = '\n'))
)
) %>%
strip_links_from_cols(c('title', 'description_bullets')) %>%
mutate_all(~ifelse(is.na(.), 'N/A', .)) %>%
glue_data(
"### {title}",
"\n\n",
"{loc}",
"\n\n",
"{institution}",
"\n\n",
"{timeline}",
"\n\n",
"{description_bullets}",
"\n\n\n",
)
}
PDF_EXPORT <- FALSE
print_section( position_data, section_id="education" )
skills <- tribble(
~skill,               ~level,
"R",                  5,
"Javascript (d3.js)", 4.5,
"Python",             4,
"Bash",               3.5,
"SQL",                3,
"C++",                3,
"AWK",                3
)
build_skill_bars(skills)
print_section( position_data, 'education' )
Education {data-icon=graduation-cap data-concise=true}
library( pagedown )
setwd()
library( pagedown )
getwd()
html_resume( "index.Rmd", css="resume", template=pkg_resource("html", "resume.html") )
library(knitr)
getwd()
html_resume( "index.Rmd", css="resume", template=pkg_resource("html", "resume.html") )
install.packages(c("knitr", "rmarkdown"))
install.packages(c("knitr", "rmarkdown"))
install.packages(c("knitr", "rmarkdown"))
install.packages(c("knitr", "rmarkdown"))
knitr::opts_chunk$set(
results='asis',
echo = FALSE
)
library(glue)
library(tidyverse)
# Set this to true to have links turned into footnotes at the end of the document
PDF_EXPORT <- FALSE
# Holds all the links that were inserted for placement at the end
links <- c()
source('parsing_functions.R')
# Load csv with position info
position_data <- read_csv('positions.csv')
# When in export mode the little dots are unaligned, so fix that.
if(PDF_EXPORT){
cat("
<style>
:root{
--decorator-outer-offset-left: -6.5px;
}
</style>")
}
# When in export mode the little dots are unaligned, so fix that.
if(PDF_EXPORT){
cat("View this CV online with links at _nickstrayer.me/cv_")
} else {
cat("[<i class='fas fa-download'></i> Download a PDF of this CV](https://github.com/nstrayer/cv/raw/master/strayer_cv.pdf)")
}
# Load csv with position info
position_data <- read_csv('positions.csv')
# Load csv with position info
position_data <- read_csv('positions.csv')
# Load csv with position info
position_data <- read_csv('positions.csv')
# Load csv with position info
position_data <- read_csv('positions.csv')
skills <- tribble(
~skill,               ~level,
"R",                        3,
"Facilitation",             5,
"Curricula Writing",        4,
"Cross-Cultural Experience",5,
"Mental Health Advocacy",   5,
"Low-Resource Adaptability",5,
"Conflict-Resolution",      5
)
build_skill_bars(skills)
# Load csv with position info
position_data <- read_csv('positions.csv')
# Load csv with position info
position_data <- read_csv('positions.csv')
skills <- tribble(
~skill,     ~level,
"R",            3,
"Facilitation", 5,
"Curricula",    4,
"Cross-Culture",5,
"Mental Health",5,
"Low-Resource", 5,
"Conflict",     5
)
build_skill_bars(skills)
library(tufte)
knitr::opts_chunk$set(
results='asis',
echo = FALSE
)
library(tufte)
getwd()
setwd(/Users/karabrick/Desktop)
setwd(Users/karabrick/Desktop)
setwd("/Users/karabrick/Desktop")
![Barclayville Classroom]("/Users/karabrick/Desktop/class.jpg")
![BarclayvilleClassroom]("/Users/karabrick/Desktop/class.jpg")
![]("/Users/karabrick/Desktop/class.jpg")
!("/Users/karabrick/Desktop/class.jpg")
![](/Users/karabrick/Desktop/class.jpg)
setwd("/Users/karabrick/Desktop")
knitr::include_graphics
knitr::include_graphics(/Users/karabrick/Desktop/class.jpg)
getwd()
setwd(/Users/karabrick/Desktop)
setwd("/Users/karabrick/Desktop")
![](/Users/karabrick/Desktop/class)
getwd()
setwd("/Users/karabrick/Desktop")
getwd()
setwd("/Users/karabrick/Desktop")
![](/Users/karabrick/Desktop/class)
# Load csv with position info
position_data <- read_csv('positions.csv')
knitr::opts_chunk$set(
results='asis',
echo = FALSE
)
library(glue)
library(tidyverse)
# Set this to true to have links turned into footnotes at the end of the document
PDF_EXPORT <- FALSE
# Holds all the links that were inserted for placement at the end
links <- c()
source('parsing_functions.R')
# Load csv with position info
position_data <- read_csv('positions.csv')
# Load csv with position info
position_data <- read_csv('positions.csv')
# Load csv with position info
position_data <- read_csv('positions.csv')
library(quanteda)
library(readtext)
dat_inT <- read.csv(paste0("/Users/karabrick/Desktop/CPP527/IN.DAT.csv"))
setwd("/Users/karabrick/Desktop/CPP527")
knitr::opts_chunk$set(root.dir = normalizePath("~/Desktop/CPP527"))
library(RColorBrewer)
library(pander)
library(dplyr)
library(quanteda)
library(readtext)
dat_inT <- read.csv(paste0("/Users/karabrick/Desktop/CPP527/IN.DAT.csv"))
dat_inT <- read.csv(paste0("/Users/karabrick/Desktop/CPP527/IN.DAT.csv"))
int.corpus <- corpus(dat_inT, text_field = "Interview")
summary(int.corpus)
docnames(int.corpus) <- int.corpus$"Interviewee"
dat_inT <- read.csv(paste0("/Users/karabrick/Desktop/CPP527/IN.DAT.csv"))
library(quanteda)
library(readtext)
dat_inT <- read.csv(paste0("/Users/karabrick/Desktop/CPP527/IN.DAT.csv"))
```{r, fig.margin = TRUE, eval= FALSE}
library(tufte)
knitr::opts_chunk$set(root.dir = normalizePath("~/Desktop/CPP527"))
library(RColorBrewer)
library(pander)
library(dplyr)
library(quanteda)
library(readtext)
dat_inT <- read.csv(paste0("/Users/karabrick/Desktop/CPP527/IN.DAT.csv"))
dat_inT <- read.csv(paste0("/Users/karabrick/Desktop/CPP527/IN.DAT.csv"))
dat_inT <- read.csv(paste0("/Users/karabrick/Desktop/CPP527/IN.DAT.csv"))
int.corpus <- corpus(dat_inT, text_field = "Interview")
summary(int.corpus)
dat_inT <- read.csv(paste0("/Users/karabrick/Desktop/CPP527/IN.DAT.csv"))
int.corpus <- corpus(dat_inT, text_field = "Interview")
int.corpus <- corpus(dat_inT, text_field = "IN.DAT")
summary(int.corpus)
docnames(int.corpus) <- int.corpus$"Interviewee"
#WordCloud
IC.col <- brewer.pal(11, "Spectral")
IC.cloud <- int.corpus %>%
dfm(remove = c('yeah', 'okay',stopwords('english')), remove_punct = TRUE) %>%
dfm_trim(verbose = FALSE)
set.seed(100)
textplot_wordcloud(max_size = 4, IC.cloud, color = IC.col)
IC.col <- brewer.pal(9, "Greens")
IC.cloud <- int.corpus %>%
dfm(remove = c('yeah', 'okay',stopwords('english')), remove_punct = TRUE) %>%
dfm_trim(verbose = FALSE)
set.seed(100)
textplot_wordcloud(max_size = 3, max_words = 50, rotation = 0, IC.cloud, color = IC.col)
int.corpus_low <- char_tolower(int.corpus)
kwic(int.corpus_low, pattern = c("challenges", phrase("mental health"))) %>%
textplot_xray(scale = "absolute")
int.corpus_low <- char_tolower(int.corpus)
int.corpus_low <- char_tolower(int.corpus)
kwic(int.corpus_low, pattern = c("challenges", phrase("mental health"))) %>%
textplot_xray(scale = "absolute")
int.corpus_low
summary(int.corpus_low)
str(int.corpus_low )
glimpse(int.corpus_low)
summary(int.corpus)
docnames(int.corpus) <- int.corpus$"Interviewee"
summary(int.corpus)
int.corpus <- corpus(dat_inT, text_field = "Interview")
int.corpus <- corpus(dat_inT, text_field = "Interview")
int.corpus <- corpus(dat_inT, text_field = "B")
int.corpus <- corpus(dat_inT, text_field = "B")
dat_inT <- read.csv(paste0("/Users/karabrick/Desktop/CPP527/IN.DAT.csv"))
int.corpus <- corpus(dat_inT, text_field = "Interview")
docnames(int.corpus) <- int.corpus$"Interviewee"
summary(int.corpus)
int.corpus_low <- char_tolower(int.corpus)
kwic(int.corpus_low, pattern = c("challenges", phrase("mental health"))) %>%
textplot_xray(scale = "absolute")
head(kwic(int.corpus_low, "student", window = 4))
int.corpus_dfm <- dfm(int.corpus_low, remove_punct = TRUE, remove = c('yeah', 'okay', 'first', stopwords('english'), stem = TRUE))
head(int.corpus_dfm, nf = 50)
this.tokens.in <- tokens(int.corpus,
remove_numbers = TRUE,
remove_separators = TRUE,
remove_punct = TRUE)
this.tokens.in<- tokens_select(this.tokens.in, stopwords('english'), selection='remove')
this.tok.in.dfm <- this.tokens.in %>% dfm(remove = 'okay')
data.frame(t(labelTopics(model.stm, n = 1)$prob))
library("stm")
topic.count <- 6
tok.dfm.trim <- dfm_trim(this.tok.in.dfm, min_docfreq = 0.075, max_docfreq = 0.90, docfreq_type = "prop") # min 7.5% / max 95%
dfm2stm <- convert(tok.dfm.trim, to = "stm")
model.stm <- stm(dfm2stm$documents, dfm2stm$vocab, K = topic.count, data = dfm2stm$meta, init.type = "Spectral")
data.frame(t(labelTopics(model.stm, n = 1)$prob))
plot(model.stm, type = "summary", text.cex = 0.5)
plot(model.stm, n = 10, type = "perspectives", topics = c(1,5))
int.corpus_low <- char_tolower(int.corpus)
int.corpus_dfm <- dfm(int.corpus_low, remove_punct = TRUE, remove = c('yeah', 'okay', 'first', stopwords('english'), stem = TRUE))
head(int.corpus_dfm, nf = 50)
int.corpus_low <- char_tolower(int.corpus)
int.corpus_dfm <- dfm(int.corpus, remove_punct = TRUE, remove = c('yeah', 'okay', 'first', stopwords('english'), stem = TRUE))
head(int.corpus_dfm, nf = 50)
# Load csv with position info
position_data <- read_csv('positions.csv')
# Load csv with position info
position_data <- read_csv('positions.csv')
gif_search(query, limit = 10, offset = 0, rating = "g",
img_format = c("fixed_height", "fixed_height_still", "fixed_height_downsampled", "fixed_width", "fixed_width_still", "fixed_width_downsampled", "fixed_height_small", "fixed_height_small_still", "fixed_width_small", "fixed_width_small_still", "downsized", "downsized_still", "downsized_large", "downsized_medium", "original", "original_still", "preview_gif", "fixed_height_mp4", "fixed_width_mp4", "fixed_height_small_mp4", "fixed_width_small_mp4", "original_mp4", "looping_mp4", "preview_mp4", "downsized_small_mp4"))
install.packages(giphyr)
install.packages("giphyr")
gif_search(query, limit = 10, offset = 0, rating = "g",
img_format = c("fixed_height", "fixed_height_still", "fixed_height_downsampled", "fixed_width", "fixed_width_still", "fixed_width_downsampled", "fixed_height_small", "fixed_height_small_still", "fixed_width_small", "fixed_width_small_still", "downsized", "downsized_still", "downsized_large", "downsized_medium", "original", "original_still", "preview_gif", "fixed_height_mp4", "fixed_width_mp4", "fixed_height_small_mp4", "fixed_width_small_mp4", "original_mp4", "looping_mp4", "preview_mp4", "downsized_small_mp4"))
library(giphyr)
gif_search("dog")
gif_search("the office")
gif_search("panda", limit = 100, img_format = "downsized_small_mp4")
panda <- gif_search("panda", limit = 100, img_format = "downsized_small_mp4")
panda [1]
panda
install.packages(gsheet2text)
install.packages("gsheet")
library(gsheet)
url <- 'docs.google.com/spreadsheets/d/1I9mJsS5QnXF2TNNntTy-HrcdHmIF9wJ8ONYvEJTXSNo' read.csv(text=gsheet2text(url, format='csv'), stringsAsFactors=FALSE)
url <- 'docs.google.com/spreadsheets/d/1I9mJsS5QnXF2TNNntTy-HrcdHmIF9wJ8ONYvEJTXSNo'
a <- gsheet2tbl(url)
a
b <- gsheet2tbl(purl)
purl <- https://drive.google.com/file/d/1ZYPgNdGXjKhh0_4XGfpgxLmjwy2z9U_Z/view?usp=sharing
purl <- drive.google.com/file/d/1ZYPgNdGXjKhh0_4XGfpgxLmjwy2z9U_Z/view?usp=sharing
purl <- 'https://drive.google.com/file/d/1ZYPgNdGXjKhh0_4XGfpgxLmjwy2z9U_Z/view?usp=sharing'
b <- gsheet2tbl(purl)
b
class(b)
library(readtext)
read_table(b)
read_file(b)
read_table2(b)
read_table2(b,stringsAsFactors = FALSE, header = TRUE, blank.lines.skip = TRUE)
read_table2(b, stringsAsFactors = FALSE, blank.lines.skip = TRUE)
read_table2(b, blank.lines.skip = TRUE)
read_table2(gsheet2tbl(purl))
read_table2(gsheet2tbl('https://drive.google.com/file/d/1ZYPgNdGXjKhh0_4XGfpgxLmjwy2z9U_Z/view?usp=sharing'))
read_table2('https://drive.google.com/file/d/1ZYPgNdGXjKhh0_4XGfpgxLmjwy2z9U_Z/view?usp=sharing')
teen.preg <- read_table2('https://drive.google.com/file/d/1ZYPgNdGXjKhh0_4XGfpgxLmjwy2z9U_Z/view?usp=sharing')
teen.preg <- read_table2('https://drive.google.com/file/d/1ZYPgNdGXjKhh0_4XGfpgxLmjwy2z9U_Z/view?usp=sharing', na = character())
str(teen.preg)
summary(teen.preg)
getwd()
getwd()
knitr::opts_chunk$set(
results='asis',
echo = FALSE
)
library(glue)
library(tidyverse)
# Set this to true to have links turned into footnotes at the end of the document
PDF_EXPORT <- FALSE
# Holds all the links that were inserted for placement at the end
links <- c()
source('parsing_functions.R')
# Load csv with position info
position_data <- read_csv('positions.csv')
# When in export mode the little dots are unaligned, so fix that.
if(PDF_EXPORT){
cat("
<style>
:root{
--decorator-outer-offset-left: -6.5px;
}
</style>")
}
# When in export mode the little dots are unaligned, so fix that.
if(PDF_EXPORT){
cat("View this CV online with links at ")
} else {
cat("[<i class='fas fa-download'></i> Download a PDF of this CV]()")
}
intro_text <- "I have coordinated and facilitated dozens of workshops, lessons and instructional practica. I am currently mastering data skills to better society and tell the stories that need to be heard.
Currently searching for a data science position that allows me to build tools using visualization and machine learning to help people explore and understand their data, problems and solutions.
"
cat(sanitize_links(intro_text))
if(PDF_EXPORT){
cat("
Links {data-icon=link}
--------------------------------------------------------------------------------
<br>
")
walk2(links, 1:length(links), function(link, index){
print(glue('{index}. {link}'))
})
}
getwd()
setwd("/Users/karabrick/Desktop/CPP527/CV.io")
# Load csv with position info
position_data <- read_csv('positions.csv')
# Load csv with position info
position_data <- read_csv('positions.csv')
